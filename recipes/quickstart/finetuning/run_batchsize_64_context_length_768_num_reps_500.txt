W0802 10:30:27.513000 22485907542336 torch/distributed/run.py:779] 
W0802 10:30:27.513000 22485907542336 torch/distributed/run.py:779] *****************************************
W0802 10:30:27.513000 22485907542336 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0802 10:30:27.513000 22485907542336 torch/distributed/run.py:779] *****************************************
/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead
  from torch.distributed._shard.checkpoint import (
/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead
  from torch.distributed._shard.checkpoint import (
/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead
  from torch.distributed._shard.checkpoint import (
/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead
  from torch.distributed._shard.checkpoint import (
Warning: unknown parameter num_reps
Warning: unknown parameter train_size
Warning: unknown parameter valid_size
Warning: unknown parameter use_speed
Warning: unknown parameter num_reps
Warning: unknown parameter train_size
Warning: unknown parameter valid_size
Warning: unknown parameter use_speed
Warning: unknown parameter num_reps
Warning: unknown parameter train_size
Warning: unknown parameter valid_size
Warning: unknown parameter use_speed
Warning: unknown parameter num_reps
Warning: unknown parameter train_size
Warning: unknown parameter valid_size
Warning: unknown parameter use_speed
Clearing GPU cache for all ranks
--> Running with torch dist debug set to detail

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.40s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.41s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.50s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.39s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.41s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.42s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.43s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.22s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.33s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.39s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.42s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.36s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.74s/it]

Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.38s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.76s/it]
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.62s/it]
--> Model models/Meta-Llama-3.1-8B-Instruct

--> models/Meta-Llama-3.1-8B-Instruct has 1050.939392 Million params

trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
--> applying fsdp activation checkpointing...
--> applying fsdp activation checkpointing...
--> applying fsdp activation checkpointing...--> applying fsdp activation checkpointing...


Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 38619.09 examples/s]

Map: 100%|██████████| 1000/1000 [00:00<00:00, 38864.94 examples/s]

Map: 100%|██████████| 1000/1000 [00:00<00:00, 38163.69 examples/s]

Map: 100%|██████████| 1000/1000 [00:00<00:00, 37990.16 examples/s]

Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]
Map:  15%|█▌        | 153/1000 [00:00<00:00, 1517.47 examples/s]
Map:  15%|█▌        | 153/1000 [00:00<00:00, 1510.48 examples/s]
Map:  15%|█▌        | 153/1000 [00:00<00:00, 1514.98 examples/s]
Map:  15%|█▌        | 152/1000 [00:00<00:00, 1507.95 examples/s]
Map:  31%|███       | 310/1000 [00:00<00:00, 1545.53 examples/s]
Map:  31%|███       | 309/1000 [00:00<00:00, 1533.59 examples/s]
Map:  31%|███       | 310/1000 [00:00<00:00, 1543.18 examples/s]
Map:  31%|███       | 308/1000 [00:00<00:00, 1532.72 examples/s]
Map:  46%|████▋     | 465/1000 [00:00<00:00, 1546.26 examples/s]
Map:  47%|████▋     | 466/1000 [00:00<00:00, 1543.85 examples/s]
Map:  47%|████▋     | 467/1000 [00:00<00:00, 1552.02 examples/s]
Map:  46%|████▋     | 463/1000 [00:00<00:00, 1532.79 examples/s]
Map:  62%|██████▏   | 623/1000 [00:00<00:00, 1556.25 examples/s]
Map:  62%|██████▎   | 625/1000 [00:00<00:00, 1555.14 examples/s]
Map:  63%|██████▎   | 627/1000 [00:00<00:00, 1565.08 examples/s]
Map:  62%|██████▏   | 621/1000 [00:00<00:00, 1548.39 examples/s]
Map:  78%|███████▊  | 784/1000 [00:00<00:00, 1562.54 examples/s]
Map:  85%|████████▌ | 853/1000 [00:00<00:00, 1544.53 examples/s]
Map:  86%|████████▌ | 855/1000 [00:00<00:00, 1539.37 examples/s]
Map:  85%|████████▌ | 850/1000 [00:00<00:00, 1535.53 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1332.23 examples/s]

Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 25738.24 examples/s]

Map: 100%|██████████| 1000/1000 [00:00<00:00, 1323.44 examples/s]
--> Training Set Length = 1000

Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 24995.85 examples/s]

Map: 100%|██████████| 1000/1000 [00:00<00:00, 1321.07 examples/s]

Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 24587.04 examples/s]

Map: 100%|██████████| 1000/1000 [00:00<00:00, 1126.00 examples/s]
Map: 100%|██████████| 1000/1000 [00:00<00:00, 1294.36 examples/s]

Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 25478.70 examples/s]

Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 1345.58 examples/s]

Preprocessing dataset:   0%|          | 0/1000 [00:00<?, ?it/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 1330.32 examples/s]
--> Validation Set Length = 100

Map: 100%|██████████| 100/100 [00:00<00:00, 1354.85 examples/s]

Preprocessing dataset:   0%|          | 0/1000 [00:00<?, ?it/s]
Preprocessing dataset:   0%|          | 0/1000 [00:00<?, ?it/s]
Map: 100%|██████████| 100/100 [00:00<00:00, 1358.07 examples/s]

Preprocessing dataset:   0%|          | 0/1000 [00:00<?, ?it/s]
Preprocessing dataset:  23%|██▎       | 228/1000 [00:00<00:00, 2271.89it/s]
Preprocessing dataset:  22%|██▎       | 225/1000 [00:00<00:00, 2243.08it/s]
Preprocessing dataset:  23%|██▎       | 228/1000 [00:00<00:00, 2269.17it/s]
Preprocessing dataset:  23%|██▎       | 227/1000 [00:00<00:00, 2267.45it/s]
Preprocessing dataset:  46%|████▌     | 456/1000 [00:00<00:00, 2267.92it/s]
Preprocessing dataset:  45%|████▌     | 450/1000 [00:00<00:00, 2232.09it/s]
Preprocessing dataset:  46%|████▌     | 455/1000 [00:00<00:00, 2262.78it/s]
Preprocessing dataset:  45%|████▌     | 454/1000 [00:00<00:00, 2249.34it/s]
Preprocessing dataset:  68%|██████▊   | 683/1000 [00:00<00:00, 2247.05it/s]
Preprocessing dataset:  67%|██████▋   | 674/1000 [00:00<00:00, 2221.73it/s]
Preprocessing dataset:  68%|██████▊   | 682/1000 [00:00<00:00, 2224.49it/s]
Preprocessing dataset:  68%|██████▊   | 679/1000 [00:00<00:00, 2233.56it/s]
Preprocessing dataset:  91%|█████████ | 908/1000 [00:00<00:00, 2228.36it/s]
Preprocessing dataset:  90%|████████▉ | 897/1000 [00:00<00:00, 2214.03it/s]
Preprocessing dataset:  90%|█████████ | 905/1000 [00:00<00:00, 2207.76it/s]
Preprocessing dataset:  90%|█████████ | 903/1000 [00:00<00:00, 2220.86it/s]
Preprocessing dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2240.62it/s]

Preprocessing dataset:   0%|          | 0/100 [00:00<?, ?it/s]
Preprocessing dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2218.54it/s]

Preprocessing dataset:   0%|          | 0/100 [00:00<?, ?it/s]
Preprocessing dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2223.16it/s]

Preprocessing dataset:   0%|          | 0/100 [00:00<?, ?it/s]
Preprocessing dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2229.21it/s]

Preprocessing dataset:   0%|          | 0/100 [00:00<?, ?it/s]
Preprocessing dataset: 100%|██████████| 100/100 [00:00<00:00, 2271.91it/s]
--> Num of Validation Set Batches loaded = 16

Preprocessing dataset: 100%|██████████| 100/100 [00:00<00:00, 2257.08it/s]
--> Num of Validation Set Batches loaded = 16

Preprocessing dataset: 100%|██████████| 100/100 [00:00<00:00, 2194.70it/s]
--> Num of Validation Set Batches loaded = 16

Preprocessing dataset: 100%|██████████| 100/100 [00:00<00:00, 2238.39it/s]
--> Num of Validation Set Batches loaded = 16
/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(

Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:00<?, ?it/s]
Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:00<?, ?it/s]
Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:00<?, ?it/s]
Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
[rank3]: Traceback (most recent call last):
[rank3]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/recipes/quickstart/finetuning/finetuning.py", line 8, in <module>
[rank3]:     fire.Fire(main)
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 143, in Fire
[rank3]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)
[rank3]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 477, in _Fire
[rank3]:     component, remaining_args = _CallAndUpdateTrace(
[rank3]:                                 ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
[rank3]:     component = fn(*varargs, **kwargs)
[rank3]:                 ^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/finetuning.py", line 269, in main
[rank3]:     results = train(
[rank3]:               ^^^^^^
[rank3]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/utils/train_utils.py", line 154, in train
[rank3]:     loss = model(**batch).loss
[rank3]:            ^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank3]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/peft_model.py", line 1577, in forward
[rank3]:     return self.base_model(
[rank3]:            ^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank3]:     return self.model.forward(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward
[rank3]:     loss = loss_fct(shift_logits, shift_labels)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1188, in forward
[rank3]:     return F.cross_entropy(input, target, weight=self.weight,
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/functional.py", line 3104, in cross_entropy
[rank3]:     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.46 GiB. GPU 3 has a total capacity of 79.44 GiB of which 9.25 GiB is free. Including non-PyTorch memory, this process has 70.18 GiB memory in use. Of the allocated memory 63.74 GiB is allocated by PyTorch, and 4.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/recipes/quickstart/finetuning/finetuning.py", line 8, in <module>
[rank1]:     fire.Fire(main)
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 143, in Fire
[rank1]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 477, in _Fire
[rank1]:     component, remaining_args = _CallAndUpdateTrace(
[rank1]:                                 ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
[rank1]:     component = fn(*varargs, **kwargs)
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/finetuning.py", line 269, in main
[rank1]:     results = train(
[rank1]:               ^^^^^^
[rank1]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/utils/train_utils.py", line 154, in train
[rank1]:     loss = model(**batch).loss
[rank1]:            ^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/peft_model.py", line 1577, in forward
[rank1]:     return self.base_model(
[rank1]:            ^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank1]:     return self.model.forward(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward
[rank1]:     loss = loss_fct(shift_logits, shift_labels)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1188, in forward
[rank1]:     return F.cross_entropy(input, target, weight=self.weight,
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/functional.py", line 3104, in cross_entropy
[rank1]:     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.46 GiB. GPU 1 has a total capacity of 79.44 GiB of which 9.17 GiB is free. Including non-PyTorch memory, this process has 70.26 GiB memory in use. Of the allocated memory 63.74 GiB is allocated by PyTorch, and 4.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/recipes/quickstart/finetuning/finetuning.py", line 8, in <module>
    fire.Fire(main)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/recipes/quickstart/finetuning/finetuning.py", line 8, in <module>
[rank2]:     fire.Fire(main)
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 143, in Fire
[rank2]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)
[rank2]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 477, in _Fire
[rank2]:     component, remaining_args = _CallAndUpdateTrace(
[rank2]:                                 ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
[rank2]:     component = fn(*varargs, **kwargs)
[rank2]:                 ^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/finetuning.py", line 269, in main
[rank2]:     results = train(
[rank2]:               ^^^^^^
[rank2]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/utils/train_utils.py", line 154, in train
[rank2]:     loss = model(**batch).loss
[rank2]:            ^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank2]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/peft_model.py", line 1577, in forward
[rank2]:     return self.base_model(
[rank2]:            ^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank2]:     return self.model.forward(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward
[rank2]:     loss = loss_fct(shift_logits, shift_labels)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1188, in forward
[rank2]:     return F.cross_entropy(input, target, weight=self.weight,
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/functional.py", line 3104, in cross_entropy
[rank2]:     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.46 GiB. GPU 2 has a total capacity of 79.44 GiB of which 9.17 GiB is free. Including non-PyTorch memory, this process has 70.26 GiB memory in use. Of the allocated memory 63.74 GiB is allocated by PyTorch, and 4.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/finetuning.py", line 269, in main
    results = train(
              ^^^^^^
  File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/utils/train_utils.py", line 154, in train
    loss = model(**batch).loss
           ^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/peft_model.py", line 1577, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward
    loss = loss_fct(shift_logits, shift_labels)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1188, in forward
    return F.cross_entropy(input, target, weight=self.weight,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/functional.py", line 3104, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.46 GiB. GPU 0 has a total capacity of 79.44 GiB of which 9.25 GiB is free. Including non-PyTorch memory, this process has 70.18 GiB memory in use. Of the allocated memory 63.74 GiB is allocated by PyTorch, and 4.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/recipes/quickstart/finetuning/finetuning.py", line 8, in <module>
[rank0]:     fire.Fire(main)
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 143, in Fire
[rank0]:     component_trace = _Fire(component, args, parsed_flag_args, context, name)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 477, in _Fire
[rank0]:     component, remaining_args = _CallAndUpdateTrace(
[rank0]:                                 ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
[rank0]:     component = fn(*varargs, **kwargs)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/finetuning.py", line 269, in main
[rank0]:     results = train(
[rank0]:               ^^^^^^
[rank0]:   File "/scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/src/llama_recipes/utils/train_utils.py", line 154, in train
[rank0]:     loss = model(**batch).loss
[rank0]:            ^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/peft_model.py", line 1577, in forward
[rank0]:     return self.base_model(
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1174, in forward
[rank0]:     loss = loss_fct(shift_logits, shift_labels)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/modules/loss.py", line 1188, in forward
[rank0]:     return F.cross_entropy(input, target, weight=self.weight,
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/nn/functional.py", line 3104, in cross_entropy
[rank0]:     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.46 GiB. GPU 0 has a total capacity of 79.44 GiB of which 9.25 GiB is free. Including non-PyTorch memory, this process has 70.18 GiB memory in use. Of the allocated memory 63.74 GiB is allocated by PyTorch, and 4.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:09<?, ?it/s]

Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:09<?, ?it/s]

Training Epoch: 1:   0%|[34m          [0m| 0/2 [00:09<?, ?it/s]
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/gpfs/vs3041/cruijff/llama-recipes-fertility/recipes/quickstart/finetuning/wandb/offline-run-20240802_103033-e88ddc4z
wandb: Find logs at: ./wandb/offline-run-20240802_103033-e88ddc4z/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
W0802 10:31:03.119000 22485907542336 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2115416 closing signal SIGTERM
W0802 10:31:03.120000 22485907542336 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2115417 closing signal SIGTERM
W0802 10:31:03.120000 22485907542336 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2115419 closing signal SIGTERM
E0802 10:31:03.584000 22485907542336 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2115418) of binary: /home/vs3041/.new_recipes/bin/python3
Traceback (most recent call last):
  File "/home/vs3041/.new_recipes/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vs3041/.new_recipes/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
finetuning.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-08-02_10:31:03
  host      : della-l09g4
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2115418)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
